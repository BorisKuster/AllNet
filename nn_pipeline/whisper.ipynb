{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc9cf248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import ffmpeg\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#whisper_model = whisper.load_model(\"base\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e087a87",
   "metadata": {},
   "source": [
    "# Get input audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c4bc7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "def record_clip(DEVICE_INDEX = 10, FORMAT = pyaudio.paInt16, RATE=48000, CHUNK=1024, RECORD_SECONDS=6,\n",
    "                WAVE_OUTPUT_FILENAME = 'filename.wav'):\n",
    "\n",
    "    audio = pyaudio.PyAudio()\n",
    "    \n",
    "    # start Recording\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                        rate=RATE, input=True,\n",
    "                        frames_per_buffer=CHUNK,\n",
    "                        input_device_index=DEVICE_INDEX)\n",
    "                        # sample_rate=RATE)\n",
    "    print(\"recording...\")\n",
    "    print('---------------------------------')\n",
    "    print(int(RATE / CHUNK * RECORD_SECONDS))\n",
    "    print('*********************************')\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        print(\"Recording . . .\")\n",
    "\n",
    "        frames.append(data)\n",
    "    print(\"Recording finished. . .\")\n",
    "\n",
    "    # stop Recording\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    waveFile.setnchannels(CHANNELS)\n",
    "    waveFile.setsampwidth(2)\n",
    "    audio.get_sample_size(FORMAT)\n",
    "    waveFile.setframerate(RATE)\n",
    "    waveFile.writeframes(b''.join(frames))\n",
    "    waveFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02b5e8e",
   "metadata": {},
   "source": [
    "# Process audio and turn to TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56803f",
   "metadata": {},
   "source": [
    "# TEXT to SPEECH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6546af9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/your_tts is already downloaded.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": " [!] Model file not found in the output path",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtts\u001b[38;5;241m.\u001b[39mtts_to_file(text\u001b[38;5;241m=\u001b[39mtext, speaker\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtts\u001b[38;5;241m.\u001b[39mspeakers[\u001b[38;5;241m3\u001b[39m], language\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtts\u001b[38;5;241m.\u001b[39mlanguages[\u001b[38;5;241m0\u001b[39m], \n\u001b[1;32m     18\u001b[0m                              file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m#wav = tts.tts(\"This is a test! This is also a test!!\", speaker=tts.speakers[1], language=tts.languages[0])\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;66;03m# Text to speech to a file\u001b[39;00m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m#tts.tts_to_file(text=\"Hello world!\", speaker=tts.speakers[0], language=tts.languages[0], file_path=\"output.wav\")\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m txt2spch \u001b[38;5;241m=\u001b[39m \u001b[43mTextToSpeech\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mTextToSpeech.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m model_name \u001b[38;5;241m=\u001b[39m TTS\u001b[38;5;241m.\u001b[39mlist_models()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Init TTS\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtts \u001b[38;5;241m=\u001b[39m \u001b[43mTTS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/TTS/api.py:54\u001b[0m, in \u001b[0;36mTTS.__init__\u001b[0;34m(self, model_name, model_path, config_path, vocoder_path, vocoder_config_path, progress_bar, gpu)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynthesizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_name:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_path:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_model_by_path(\n\u001b[1;32m     57\u001b[0m         model_path, config_path, vocoder_path\u001b[38;5;241m=\u001b[39mvocoder_path, vocoder_config\u001b[38;5;241m=\u001b[39mvocoder_config_path, gpu\u001b[38;5;241m=\u001b[39mgpu\n\u001b[1;32m     58\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/TTS/api.py:114\u001b[0m, in \u001b[0;36mTTS.load_model_by_name\u001b[0;34m(self, model_name, gpu)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name: \u001b[38;5;28mstr\u001b[39m, gpu: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    105\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load one of 🐸TTS models by name.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    TODO: Add tests\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     model_path, config_path, vocoder_path, vocoder_config_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_model_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# init synthesizer\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# None values are fetch from the model\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynthesizer \u001b[38;5;241m=\u001b[39m Synthesizer(\n\u001b[1;32m    119\u001b[0m         tts_checkpoint\u001b[38;5;241m=\u001b[39mmodel_path,\n\u001b[1;32m    120\u001b[0m         tts_config_path\u001b[38;5;241m=\u001b[39mconfig_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m         use_cuda\u001b[38;5;241m=\u001b[39mgpu,\n\u001b[1;32m    128\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/TTS/api.py:98\u001b[0m, in \u001b[0;36mTTS.download_model_by_name\u001b[0;34m(self, model_name)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_model_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 98\u001b[0m     model_path, config_path, model_item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault_vocoder\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model_path, config_path, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/TTS/utils/manage.py:248\u001b[0m, in \u001b[0;36mModelManager.download_model\u001b[0;34m(self, model_name)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_model_license(model_item\u001b[38;5;241m=\u001b[39mmodel_item)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# find downloaded files\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m output_model_path, output_config_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# update paths in the config.json\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_paths(output_path, output_config_path)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/TTS/utils/manage.py:271\u001b[0m, in \u001b[0;36mModelManager._find_files\u001b[0;34m(output_path)\u001b[0m\n\u001b[1;32m    269\u001b[0m         config_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_path, file_name)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m [!] Model file not found in the output path\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m [!] Config file not found in the output path\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m:  [!] Model file not found in the output path"
     ]
    }
   ],
   "source": [
    "from TTS.api import TTS\n",
    "\n",
    "class TextToSpeech:\n",
    "    def __init__(self):\n",
    "        \n",
    "\n",
    "        # Running a multi-speaker and multi-lingual model\n",
    "\n",
    "        # List available \n",
    "        model_name = TTS.list_models()[0]\n",
    "        # Init TTS\n",
    "        self.tts = TTS(model_name, progress_bar = False, gpu = True)\n",
    "    def text_to_speech():\n",
    "        0\n",
    "    def text_to_speech_to_file(self, text, output = 'output.wav'):\n",
    "        # Run TTS\n",
    "        self.tts.tts_to_file(text=text, speaker=self.tts.speakers[3], language=self.tts.languages[0], \n",
    "                             file_path=\"output.wav\")\n",
    "        #wav = tts.tts(\"This is a test! This is also a test!!\", speaker=tts.speakers[1], language=tts.languages[0])\n",
    "        # Text to speech to a file\n",
    "        #tts.tts_to_file(text=\"Hello world!\", speaker=tts.speakers[0], language=tts.languages[0], file_path=\"output.wav\")\n",
    "txt2spch = TextToSpeech()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896b0b95",
   "metadata": {},
   "source": [
    "# ChatGPT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc31b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Get a ChatGPT API which actually works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1048e3a0",
   "metadata": {},
   "source": [
    "# Streaming audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0154ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, logging\n",
    "from datetime import datetime\n",
    "import threading, collections, queue, os, os.path\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "import wave\n",
    "import webrtcvad\n",
    "from halo import Halo\n",
    "from scipy import signal\n",
    "\n",
    "logging.basicConfig(level=20)\n",
    "\n",
    "class Audio(object):\n",
    "    \"\"\"Streams raw audio from microphone. Data is received in a separate thread, and stored in a buffer, to be read from.\"\"\"\n",
    "\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    # Network/VAD rate-space\n",
    "    RATE_PROCESS = 16000\n",
    "    CHANNELS = 1\n",
    "    BLOCKS_PER_SECOND = 50\n",
    "\n",
    "    def __init__(self, callback=None, device=None, input_rate=RATE_PROCESS, file=None):\n",
    "        def proxy_callback(in_data, frame_count, time_info, status):\n",
    "            #pylint: disable=unused-argument\n",
    "            if self.chunk is not None:\n",
    "                in_data = self.wf.readframes(self.chunk)\n",
    "            callback(in_data)\n",
    "            return (None, pyaudio.paContinue)\n",
    "        if callback is None: callback = lambda in_data: self.buffer_queue.put(in_data)\n",
    "        self.buffer_queue = queue.Queue()\n",
    "        self.device = device\n",
    "        self.input_rate = input_rate\n",
    "        self.sample_rate = self.RATE_PROCESS\n",
    "        self.block_size = int(self.RATE_PROCESS / float(self.BLOCKS_PER_SECOND))\n",
    "        self.block_size_input = int(self.input_rate / float(self.BLOCKS_PER_SECOND))\n",
    "        self.pa = pyaudio.PyAudio()\n",
    "\n",
    "        kwargs = {\n",
    "            'format': self.FORMAT,\n",
    "            'channels': self.CHANNELS,\n",
    "            'rate': self.input_rate,\n",
    "            'input': True,\n",
    "            'frames_per_buffer': self.block_size_input,\n",
    "            'stream_callback': proxy_callback,\n",
    "        }\n",
    "\n",
    "        self.chunk = None\n",
    "        # if not default device\n",
    "        if self.device:\n",
    "            kwargs['input_device_index'] = self.device\n",
    "        elif file is not None:\n",
    "            self.chunk = 320\n",
    "            self.wf = wave.open(file, 'rb')\n",
    "        print(\"KWARGS:\", kwargs)\n",
    "        self.stream = self.pa.open(**kwargs)\n",
    "        self.stream.start_stream()\n",
    "\n",
    "    def resample(self, data, input_rate):\n",
    "        \"\"\"\n",
    "        Microphone may not support our native processing sampling rate, so\n",
    "        resample from input_rate to RATE_PROCESS here for webrtcvad and\n",
    "        deepspeech\n",
    "        Args:\n",
    "            data (binary): Input audio stream\n",
    "            input_rate (int): Input audio rate to resample from\n",
    "        \"\"\"\n",
    "        data16 = np.fromstring(string=data, dtype=np.int16)\n",
    "        resample_size = int(len(data16) / self.input_rate * self.RATE_PROCESS)\n",
    "        resample = signal.resample(data16, resample_size)\n",
    "        resample16 = np.array(resample, dtype=np.int16)\n",
    "        return resample16.tostring()\n",
    "\n",
    "    def read_resampled(self):\n",
    "        \"\"\"Return a block of audio data resampled to 16000hz, blocking if necessary.\"\"\"\n",
    "        return self.resample(data=self.buffer_queue.get(),\n",
    "                             input_rate=self.input_rate)\n",
    "\n",
    "    def read(self):\n",
    "        \"\"\"Return a block of audio data, blocking if necessary.\"\"\"\n",
    "        return self.buffer_queue.get()\n",
    "\n",
    "    def destroy(self):\n",
    "        self.stream.stop_stream()\n",
    "        self.stream.close()\n",
    "        self.pa.terminate()\n",
    "\n",
    "    frame_duration_ms = property(lambda self: 1000 * self.block_size // self.sample_rate)\n",
    "\n",
    "    def write_wav(self, filename, data):\n",
    "        logging.info(\"write wav %s\", filename)\n",
    "        wf = wave.open(filename, 'wb')\n",
    "        wf.setnchannels(self.CHANNELS)\n",
    "        # wf.setsampwidth(self.pa.get_sample_size(FORMAT))\n",
    "        assert self.FORMAT == pyaudio.paInt16\n",
    "        wf.setsampwidth(2)\n",
    "        wf.setframerate(self.sample_rate)\n",
    "        wf.writeframes(data)\n",
    "        wf.close()\n",
    "\n",
    "\n",
    "class VADAudio(Audio):\n",
    "    \"\"\"Filter & segment audio with voice activity detection.\"\"\"\n",
    "\n",
    "    def __init__(self, aggressiveness=3, device=None, input_rate=None, file=None):\n",
    "        super().__init__(device=device, input_rate=input_rate, file=file)\n",
    "        self.vad = webrtcvad.Vad(aggressiveness)\n",
    "\n",
    "    def frame_generator(self):\n",
    "        \"\"\"Generator that yields all audio frames from microphone.\"\"\"\n",
    "        if self.input_rate == self.RATE_PROCESS:\n",
    "            while True:\n",
    "                yield self.read()\n",
    "        else:\n",
    "            while True:\n",
    "                yield self.read_resampled()\n",
    "\n",
    "    def vad_collector(self, padding_ms=300, ratio=0.75, frames=None):\n",
    "        \"\"\"Generator that yields series of consecutive audio frames comprising each utterence, separated by yielding a single None.\n",
    "            Determines voice activity by ratio of frames in padding_ms. Uses a buffer to include padding_ms prior to being triggered.\n",
    "            Example: (frame, ..., frame, None, frame, ..., frame, None, ...)\n",
    "                      |---utterence---|        |---utterence---|\n",
    "        \"\"\"\n",
    "        if frames is None: frames = self.frame_generator()\n",
    "        num_padding_frames = padding_ms // self.frame_duration_ms\n",
    "        ring_buffer = collections.deque(maxlen=num_padding_frames)\n",
    "        triggered = False\n",
    "\n",
    "        for frame in frames:\n",
    "            if len(frame) < 640:\n",
    "                return\n",
    "\n",
    "            is_speech = self.vad.is_speech(frame, self.sample_rate)\n",
    "\n",
    "            if not triggered:\n",
    "                ring_buffer.append((frame, is_speech))\n",
    "                num_voiced = len([f for f, speech in ring_buffer if speech])\n",
    "                if num_voiced > ratio * ring_buffer.maxlen:\n",
    "                    triggered = True\n",
    "                    for f, s in ring_buffer:\n",
    "                        yield f\n",
    "                    ring_buffer.clear()\n",
    "\n",
    "            else:\n",
    "                yield frame\n",
    "                ring_buffer.append((frame, is_speech))\n",
    "                num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n",
    "                if num_unvoiced > ratio * ring_buffer.maxlen:\n",
    "                    triggered = False\n",
    "                    yield None\n",
    "                    ring_buffer.clear()\n",
    "\n",
    "def main(nospinner = True, model=None, savewav = False, vad_aggressiveness=3, device = 7, rate = 48000,\n",
    "        file = 'out.wav'):\n",
    "    \n",
    "    # Start audio with VAD\n",
    "    vad_audio = VADAudio(aggressiveness=vad_aggressiveness,\n",
    "                         device=device,\n",
    "                         input_rate=rate,\n",
    "                         file=file)\n",
    "    print(\"Listening (ctrl-C to exit)...\")\n",
    "    frames = vad_audio.vad_collector()\n",
    "\n",
    "    # Stream from microphone to the model using VAD\n",
    "    spinner = None\n",
    "    if not nospinner:\n",
    "        spinner = Halo(spinner='line')\n",
    "    #stream_context = model.createStream()\n",
    "    wav_data = bytearray()\n",
    "    for frame in frames:\n",
    "        if frame is not None:\n",
    "            if spinner: spinner.start()\n",
    "            logging.debug(\"streaming frame\")\n",
    "            #stream_context.feedAudioContent(np.frombuffer(frame, np.int16))\n",
    "            if savewav: wav_data.extend(frame)\n",
    "        else:\n",
    "            if spinner: spinner.stop()\n",
    "            logging.debug(\"end utterance\")\n",
    "            if savewav:\n",
    "                vad_audio.write_wav(os.path.join(savewav, datetime.now().strftime(\"savewav_%Y-%m-%d_%H-%M-%S_%f.wav\")), wav_data)\n",
    "                wav_data = bytearray()\n",
    "            #text = stream_context.finishStream()\n",
    "            #print(\"Recognized: %s\" % text)\n",
    "            #stream_context = model.createStream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3909b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyaudio_get_input_mic_device(searching_for = \"Logitech\"):\n",
    "    # Input params\n",
    "    # Searching for - partial name of device which should be used. UPPER AND LOWER CASE MATTERS ! \n",
    "\n",
    "    # Find device\n",
    "    good_device_info = None\n",
    "    aud = pyaudio.PyAudio()\n",
    "    for i in range(0,15):\n",
    "        try:\n",
    "            info = aud.get_device_info_by_index(i)\n",
    "            #print(info['name'])\n",
    "            if searching_for in info['name']:\n",
    "                print(info)\n",
    "                good_device_info = info\n",
    "        except Exception as e :\n",
    "            print(\"Exception:\", e)\n",
    "            \n",
    "    aud.terminate()\n",
    "    if good_device_info is None:\n",
    "        raise Exception(\"Did not find a good input device !\")\n",
    "    else:\n",
    "        return good_device_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1557f674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 10, 'structVersion': 2, 'name': 'Logitech USB Headset: Audio (hw:2,0)', 'hostApi': 0, 'maxInputChannels': 1, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.008, 'defaultLowOutputLatency': 0.008, 'defaultHighInputLatency': 0.032, 'defaultHighOutputLatency': 0.032, 'defaultSampleRate': 48000.0}\n",
      "KWARGS: {'format': 8, 'channels': 1, 'rate': 48000, 'input': True, 'frames_per_buffer': 960, 'stream_callback': <function Audio.__init__.<locals>.proxy_callback at 0x7fcfa0193310>, 'input_device_index': 10}\n",
      "Listening (ctrl-C to exit)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib pcm.c:2642:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2642:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2642:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_route.c:869:(find_matching_chmap) Found no matching channel map\n",
      "ALSA lib pcm.c:2642:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2642:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2642:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_route.c:869:(find_matching_chmap) Found no matching channel map\n",
      "/tmp/ipykernel_379/1496248536.py:67: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  data16 = np.fromstring(string=data, dtype=np.int16)\n",
      "/tmp/ipykernel_379/1496248536.py:71: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  return resample16.tostring()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m device_info \u001b[38;5;241m=\u001b[39m pyaudio_get_input_mic_device()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize VAD and stuffs\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnospinner\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msavewav\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvad_aggressiveness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevice_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdefaultSampleRate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mout.wav\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 169\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(nospinner, model, savewav, vad_aggressiveness, device, rate, file)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m#stream_context = model.createStream()\u001b[39;00m\n\u001b[1;32m    168\u001b[0m wav_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m()\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m frames:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m spinner: spinner\u001b[38;5;241m.\u001b[39mstart()\n",
      "Cell \u001b[0;32mIn[1], line 128\u001b[0m, in \u001b[0;36mVADAudio.vad_collector\u001b[0;34m(self, padding_ms, ratio, frames)\u001b[0m\n\u001b[1;32m    125\u001b[0m ring_buffer \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdeque(maxlen\u001b[38;5;241m=\u001b[39mnum_padding_frames)\n\u001b[1;32m    126\u001b[0m triggered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m frames:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(frame) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m640\u001b[39m:\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 115\u001b[0m, in \u001b[0;36mVADAudio.frame_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_resampled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 75\u001b[0m, in \u001b[0;36mAudio.read_resampled\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_resampled\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     74\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a block of audio data resampled to 16000hz, blocking if necessary.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresample(data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     76\u001b[0m                          input_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_rate)\n",
      "File \u001b[0;32m/usr/lib/python3.8/queue.py:170\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get good device info\n",
    "device_info = pyaudio_get_input_mic_device()\n",
    "# Initialize VAD and stuffs\n",
    "main(nospinner = True, model=None, savewav = False, vad_aggressiveness=3, device = device_info['index'], rate = int(device_info['defaultSampleRate']),\n",
    "        file = 'out.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "defae17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw = {'format': 8, 'channels': 1, 'rate': 48000, 'input': True, 'frames_per_buffer': 960, 'stream_callback': None,\n",
    "      'input_device_index': 10}\n",
    "aud = pyaudio.PyAudio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc9b0618",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must specify an input or output stream.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[43maud\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_device_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkw\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_device_index\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkw\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkw\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchannels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkw\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyaudio/__init__.py:639\u001b[0m, in \u001b[0;36mPyAudio.open\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    632\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Opens a new stream.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m    See constructor for :py:func:`PyAudio.Stream.__init__` for parameter\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m    :returns: A new :py:class:`PyAudio.Stream`\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 639\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[43mPyAudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streams\u001b[38;5;241m.\u001b[39madd(stream)\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyaudio/__init__.py:405\u001b[0m, in \u001b[0;36mPyAudio.Stream.__init__\u001b[0;34m(self, PA_manager, rate, channels, format, input, output, input_device_index, output_device_index, frames_per_buffer, start, input_host_api_specific_stream_info, output_host_api_specific_stream_info, stream_callback)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize an audio stream.\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03mDo not call directly. Use :py:func:`PyAudio.open`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m:raise ValueError: Neither input nor output are set True.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m output):\n\u001b[0;32m--> 405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust specify an input or output \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent \u001b[38;5;241m=\u001b[39m PA_manager\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Must specify an input or output stream."
     ]
    }
   ],
   "source": [
    "stream = aud.open(input_device_index = kw['input_device_index'], rate = kw['rate'], channels = kw['channels'],\n",
    "                 format = kw['format'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb29130",
   "metadata": {},
   "source": [
    "### Whisper testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1da544ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en\n",
      "I'm so much for you.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm so much for you.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def whisper_to_text(whisper_model = None, audiofile = None, steamed_frame = None):\n",
    "    if audiofile is not None:\n",
    "    # load audio and pad/trim it to fit 30 seconds\n",
    "        audio = whisper.load_audio(audiofile)\n",
    "        \n",
    "    else:\n",
    "        audio = steamed_frame\n",
    "        0\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "    # make log-Mel spectrogram and move to the same device as the model\n",
    "    mel = whisper.log_mel_spectrogram(audio).to(whisper_model.device)\n",
    "\n",
    "    # detect the spoken language\n",
    "    _, probs = whisper_model.detect_language(mel)\n",
    "    print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "\n",
    "    # decode the audio\n",
    "    options = whisper.DecodingOptions(fp16 = False, language = 'english')\n",
    "    result = whisper.decode(whisper_model, mel, options)\n",
    "\n",
    "    # print the recognized text\n",
    "    print(result.text)\n",
    "    output = result.text\n",
    "    return output\n",
    "\n",
    "whisper_to_text(audiofile =None, whisper_model = whisper_model, steamed_frame = out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b73ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 48000\n",
    "\n",
    "try:\n",
    "    # This launches a subprocess to decode audio while down-mixing and resampling as necessary.\n",
    "    # Requires the ffmpeg CLI and `ffmpeg-python` package to be installed.\n",
    "    out, _ = (\n",
    "        ffmpeg.input('filename', threads=0)\n",
    "        .output(\"-\", format=\"s16le\", acodec=\"pcm_s16le\", ac=1, ar=sr)\n",
    "        .run(cmd=[\"ffmpeg\", \"-nostdin\"], capture_stdout=True, capture_stderr=True)\n",
    "    )\n",
    "except ffmpeg.Error as e:\n",
    "    raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\") from e\n",
    "\n",
    "out = np.frombuffer(out, np.int16).flatten().astype(np.float32) / 32768.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25e1704b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(287744,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f824ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def streamed_frame_to_whisper():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
